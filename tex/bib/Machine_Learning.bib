% This file was created with JabRef 2.10.
% Encoding: ISO8859_1


@Article{Faber-Lilienfeld.JCTC.2017.13,
  Title                    = {Prediction Errors of Molecular Machine Learning Models Lower than Hybrid DFT Error},
  Author                   = {Faber, Felix A. and Hutchison, Luke and Huang, Bing and Gilmer, Justin and Schoenholz, Samuel S. and Dahl, George E. and Vinyals, Oriol and Kearnes, Steven and Riley, Patrick F. and von Lilienfeld, O. Anatole},
  Journal                  = {J. Chem. Theory Comput.},
  Year                     = {2017},

  Month                    = {oct},
  Number                   = {11},
  Pages                    = {5255--5264},
  Volume                   = {13},

  Comment                  = {Molecule - Property calculation},
  Doi                      = {10.1021/acs.jctc.7b00577},
  Eprint                   = {http://dx.doi.org/10.1021/acs.jctc.7b00577},
  File                     = {:Machine_Learning\\Faber-Lilienfeld.JCTC.2017.ASAP.pdf:PDF;:Machine_Learning\\Faber-Lilienfeld.JCTC.2017.ASAP-SI.pdf:PDF},
  Publisher                = {American Chemical Society ({ACS})},
  Timestamp                = {2018.01.29},
  Url                      = {http://dx.doi.org/10.1021/acs.jctc.7b00577}
}

@InProceedings{Gilmer-Dahl.ICoML.2017.70,
  Title                    = {Neural Message Passing for Quantum Chemistry},
  Author                   = {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
  Booktitle                = {Proceedings of the 34th International Conference on Machine Learning},
  Year                     = {2017},

  Address                  = {International Convention Centre, Sydney, Australia},
  Editor                   = {Doina Precup and Yee Whye Teh},
  Month                    = {06--11 Aug},
  Pages                    = {1263--1272},
  Publisher                = {PMLR},
  Series                   = {Proceedings of Machine Learning Research},
  Volume                   = {70},

  Abstract                 = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  File                     = {:Machine_Learning\\Gilmer-Dahl.ICoML.2017.70.pdf:PDF;:Machine_Learning\\Gilmer-Dahl.ICoML.2017.70-SI.pdf:PDF;gilmer17a.pdf:http\://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf:PDF},
  Journal                  = {International Conference on Machine Learning},
  Url                      = {http://proceedings.mlr.press/v70/gilmer17a.html}
}

@Article{Li-Zemel.ICoLR.2016,
  Title                    = {Gated Graph Sequence Neural Networks},
  Author                   = {Yujia Li and Daniel Tarlow and Marc Brockschmidt and Richard Zemel},
  Journal                  = {International Conference on Learning Representations},
  Year                     = {2016},

  __markedentry            = {[ajz34:1]},
  Abstract                 = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.},
  Date                     = {2015-11-17},
  Eprint                   = {1511.05493v4},
  Eprintclass              = {cs.LG},
  Eprinttype               = {arXiv},
  File                     = {:Machine_Learning\\Li-Zemel.ICoLR.2016.pdf:PDF},
  Keywords                 = {cs.LG, cs.AI, cs.NE, stat.ML},
  Url                      = {https://arxiv.org/abs/1511.05493}
}

@Article{Rupp-Lilienfeld.PRL.2012.108,
  Title                    = {Fast and Accurate Modeling of Molecular Atomization Energies with Machine Learning},
  Author                   = {Matthias Rupp and Alexandre Tkatchenko and Klaus-Robert MÃ¼ller and O. Anatole von Lilienfeld},
  Journal                  = {Phys. Rev. Lett.},
  Year                     = {2012},

  Month                    = {jan},
  Number                   = {5},
  Pages                    = {058301},
  Volume                   = {108},

  Comment                  = {ChemInfo - CM},
  Doi                      = {10.1103/physrevlett.108.058301},
  File                     = {:Computational_Chemistry\\Rupp-Lilienfeld.PRL.2012.108.pdf:PDF},
  Owner                    = {a},
  Publisher                = {American Physical Society ({APS})},
  Timestamp                = {2018.01.29}
}

@comment{jabref-meta: databaseType:bibtex;}

